{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfaf1652-b0ac-40bd-b011-0dbf1faaa1a1",
   "metadata": {},
   "source": [
    "# Extracting the links from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db9fe18-c060-45f9-8b81-85d1fd3a0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# excluded extension types\n",
    "EXCLUDED_EXTENSIONS = {\".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".webp\", \".zip\", \".exe\", \".mp4\", \".mp3\"}\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A utility class to scrape a website and extract only internal valid links.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.base_domain = urlparse(url).netloc  # Extract base domain (e.g., www.iiitnr.ac.in)\n",
    "        self.links = []\n",
    "        self.scrape_website()\n",
    "\n",
    "    def scrape_website(self):\n",
    "        \"\"\"\n",
    "        Fetches the webpage, parses it, and extracts only internal links.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=HEADERS, verify=False)\n",
    "            response.raise_for_status()  # Raises an error for HTTP issues\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract all <a> tags with href attribute\n",
    "            raw_links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "            self.links = self.clean_links(raw_links)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {self.url}: {e}\")\n",
    "\n",
    "    def clean_links(self, links):\n",
    "        \"\"\"\n",
    "        Cleans and filters extracted links:\n",
    "        - Converts relative links to absolute\n",
    "        - Removes fragments (#) and unwanted patterns\n",
    "        - Filters out links that don't belong to the same base domain\n",
    "        - Excludes links ending with undesired extensions\n",
    "        \"\"\"\n",
    "        valid_links = []\n",
    "        for link in links:\n",
    "            # Convert relative URL to absolute\n",
    "            absolute_url = urljoin(self.url, link)\n",
    "\n",
    "            # Parse URL components\n",
    "            parsed_url = urlparse(absolute_url)\n",
    "\n",
    "            # Ensure the link belongs to the same base domain\n",
    "            if parsed_url.netloc and parsed_url.netloc != self.base_domain:\n",
    "                continue\n",
    "\n",
    "            # Remove empty links or links starting with \"#\", \"/\", or \".\"\n",
    "            if absolute_url in (\"#\", \"/\", \".\") or link.startswith((\"#\", \"/\", \".\")):\n",
    "                continue\n",
    "\n",
    "            # Exclude links with undesired extensions\n",
    "            if any(parsed_url.path.lower().endswith(ext) for ext in EXCLUDED_EXTENSIONS):\n",
    "                continue\n",
    "\n",
    "            if not absolute_url.startswith((\"http://\", \"https://\")):\n",
    "                continue  # Skip mailto, tel, javascript links\n",
    "\n",
    "\n",
    "            valid_links.append(absolute_url)\n",
    "\n",
    "        return list(set(valid_links))  # Remove duplicates\n",
    "\n",
    "    def get_links(self):\n",
    "        \"\"\"\n",
    "        Returns the list of clean, valid, and internal links.\n",
    "        \"\"\"\n",
    "        return self.links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9092180-e229-4789-a74d-29f53b5a6f3c",
   "metadata": {},
   "source": [
    "# Selecting relevant Links with the help of LLAMA 3.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5fd3ac-b976-4b73-b375-11640598c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4c9642-2028-4de6-981b-976527a05242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links_with_llama(links):\n",
    "    \"\"\"\n",
    "    Sends extracted links to LLaMA 3.2 (via Ollama) and asks it to select the most relevant ones.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping to create a company brochure. \n",
    "    Here is a list of web links:\n",
    "\n",
    "    {json.dumps(links, indent=2)}\n",
    "\n",
    "    Select only the most relevant links for the brochure, such as:\n",
    "    - About Us\n",
    "    - Vision & Mission\n",
    "    - Leadership (Director, Officers, etc.)\n",
    "    - Contact Information\n",
    "    - Investor Relations (if available)\n",
    "    - Careers (if available)\n",
    "    - Services/Products\n",
    "\n",
    "    Do NOT include links related to privacy policies, login pages, or irrelevant content.\n",
    "\n",
    "    Return ONLY a **plain list of links** (one per line) with NO extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Query LLaMA 3.2\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    # # Debug: Print the full response from LLaMA\n",
    "    # print(\"Raw LLaMA Response:\", response.message.content)\n",
    "\n",
    "    # Extract links from response\n",
    "    links = re.findall(r'https?://\\S+', response.message.content)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9247ba0-1cd4-434d-8d9f-c9df0cc27923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\llms\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'abc.xyz'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Brochure Links: ['http://abc.xyz/investor']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://abc.xyz/\"\n",
    "scraper = WebScraper(url)\n",
    "extracted_links = scraper.links\n",
    "\n",
    "filtered_links = filter_links_with_llama(extracted_links)\n",
    "\n",
    "# Print the final selected links\n",
    "print(\"Filtered Brochure Links:\", filtered_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5d011-6a17-4ef4-950e-5631d56cdc2c",
   "metadata": {},
   "source": [
    "# Fetching content from selected links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff6ca45-9332-4483-ac2a-c1ed4b5c3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_content(url, max_chars=50000):\n",
    "    \"\"\"Fetches and cleans text content from a given webpage.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        if soup.body:\n",
    "            for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                tag.decompose()\n",
    "            text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "            return text[:max_chars]  # Truncate if needed\n",
    "        return \"\"\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62994c9-5c50-475a-822b-6c15889bb04b",
   "metadata": {},
   "source": [
    "# Generating Brochure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4f18cf-f632-4623-973a-575dce663c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brochure(company_name, links):\n",
    "    \"\"\"Generates a company brochure using LLaMA 3.2B based on extracted webpage content.\"\"\"\n",
    "    \n",
    "    # Fetch content from each link\n",
    "    all_content = []\n",
    "    for link in links:\n",
    "        print(f\"Fetching content from: {link}\")\n",
    "        page_text = fetch_page_content(link)\n",
    "        if page_text:\n",
    "            all_content.append(f\"### Source: {link}\\n{page_text}\\n\")\n",
    "\n",
    "    # Combine all content, ensuring it fits within context size\n",
    "    combined_text = \"\\n\".join(all_content)[:500000]  # Keep within ~500K characters (safe margin)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant tasked with creating a professional brochure for \"{company_name}\".\n",
    "    Below are extracts from various web pages related to the company.\n",
    "\n",
    "    Please generate a well-structured, engaging brochure suitable for customers, investors, and stakeholders.\n",
    "    Ensure it includes:\n",
    "    - A brief company introduction\n",
    "    - Key offerings/services\n",
    "    - Achievements or unique points\n",
    "    - Contact details (if available)\n",
    "    - Any other relevant sections\n",
    "\n",
    "    Here is the extracted content:\n",
    "    {combined_text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send to LLaMA 3.2B\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a246ab-3d6a-420f-9ffd-d5eeeae9c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from: http://abc.xyz/investor\n"
     ]
    }
   ],
   "source": [
    "company_name = \"Google\"\n",
    "links=filtered_links\n",
    "brochure_text = generate_brochure(company_name, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e28e49-cfae-40c5-abd3-17191dab8f67",
   "metadata": {},
   "source": [
    "# BROCHURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12964b33-ae98-409d-a643-8c3b8573c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77da3134-bc3e-4213-854e-8624a20d804f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a professionally designed brochure for Google:\n",
       "\n",
       "[Cover Page: A stylized logo of Google]\n",
       "\n",
       "**Unlocking Innovation and Connection**\n",
       "\n",
       "Welcome to Google, the world's most innovative technology company. Our mission is to organize the world's information and make it universally accessible and useful.\n",
       "\n",
       "**Our Story**\n",
       "\n",
       "Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University. Our early success was fueled by our search engine, which quickly became the go-to platform for finding answers online. Today, we're a global leader in technology, with a diverse range of products and services that connect people, businesses, and ideas.\n",
       "\n",
       "**Our Offerings**\n",
       "\n",
       "We offer a wide range of products and services that aim to make the world a better place:\n",
       "\n",
       "* **Search**: Our search engine is still the most popular platform for finding answers online.\n",
       "* **Advertising**: We provide effective advertising solutions for businesses of all sizes.\n",
       "* **Cloud Computing**: Our Google Cloud Platform offers a comprehensive suite of cloud-based services for businesses and developers.\n",
       "* **Artificial Intelligence**: Our AI-powered products and services are transforming industries and revolutionizing the way we live and work.\n",
       "* **Google Play**: Our app store provides access to millions of apps, games, and entertainment content.\n",
       "\n",
       "**Achievements**\n",
       "\n",
       "We've achieved numerous milestones and awards throughout our history, including:\n",
       "\n",
       "* **Google's Market Value**: Over $1 trillion\n",
       "* **Number of Employees**: Over 150,000 worldwide\n",
       "* **Awards**: We've won over 200 patents and have been recognized as one of the world's most innovative companies by Forbes.\n",
       "\n",
       "**Unique Points**\n",
       "\n",
       "We're committed to making a positive impact on society. Here are some unique points about Google:\n",
       "\n",
       "* **Sustainable Energy**: We aim to power 100% of our data centers with renewable energy.\n",
       "* **Education**: We provide free online courses and resources for people around the world.\n",
       "* **Accessibility**: We strive to make our products and services accessible to everyone, regardless of ability or disability.\n",
       "\n",
       "**Stay Connected**\n",
       "\n",
       "Want to stay up-to-date on the latest Google news and announcements? Subscribe to our email alerts and follow us on social media:\n",
       "\n",
       "* **Email Alerts**: [Subscribe]\n",
       "* **Social Media**: Follow us on Twitter, Facebook, Instagram, and LinkedIn.\n",
       "\n",
       "[Back Cover: A stylized logo of Google]\n",
       "\n",
       "**About Us**\n",
       "\n",
       "Google is a subsidiary of Alphabet Inc., a holding company that owns multiple companies, including Google, YouTube, and Waymo. We're headquartered in Mountain View, California, with offices all around the world.\n",
       "\n",
       "**Contact Us**\n",
       "\n",
       "If you have any questions or feedback about Google, please don't hesitate to reach out:\n",
       "\n",
       "* **Phone**: +1 (650) 253-0000\n",
       "* **Email**: [info@google.com](mailto:info@google.com)\n",
       "* **Address**: 1600 Amphitheatre Parkway, Mountain View, CA 94043"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(brochure_text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27d990-b6a7-4c95-829b-87cf9fbf348c",
   "metadata": {},
   "source": [
    "# Note\n",
    "* Note that we have used LLAMA 3.2 trained on 3.2 Billion parameters.\n",
    "* So it may not be as powerful as GPT-4 model by open AI which is trained on 1.8 trillion parameters\n",
    "* When we use open AI api based model it gives results in different format with proper markdowns and headings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a7470-cbdf-49f6-9ffe-13e88ecef502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
