{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfaf1652-b0ac-40bd-b011-0dbf1faaa1a1",
   "metadata": {},
   "source": [
    "# Extracting the links from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db9fe18-c060-45f9-8b81-85d1fd3a0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# excluded extension types\n",
    "EXCLUDED_EXTENSIONS = {\".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".webp\", \".zip\", \".exe\", \".mp4\", \".mp3\"}\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    A utility class to scrape a website and extract only internal valid links.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.base_domain = urlparse(url).netloc  \n",
    "        self.links = []\n",
    "        self.scrape_website()\n",
    "\n",
    "    def scrape_website(self):\n",
    "        \"\"\"\n",
    "        Fetches the webpage, parses it, and extracts only internal links.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=HEADERS, verify=False)\n",
    "            response.raise_for_status()  \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract all <a> tags with href attribute\n",
    "            raw_links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "            self.links = self.clean_links(raw_links)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {self.url}: {e}\")\n",
    "\n",
    "    def clean_links(self, links):\n",
    "        \"\"\"\n",
    "        Cleans and filters extracted links:\n",
    "        - Converts relative links to absolute\n",
    "        - Removes fragments (#) and unwanted patterns\n",
    "        - Filters out links that don't belong to the same base domain\n",
    "        - Excludes links ending with undesired extensions\n",
    "        \"\"\"\n",
    "        valid_links = []\n",
    "        for link in links:\n",
    "            # relative URL to absolute\n",
    "            absolute_url = urljoin(self.url, link)\n",
    "            \n",
    "            parsed_url = urlparse(absolute_url)\n",
    "\n",
    "            # Ensures link belongs to base domain\n",
    "            if parsed_url.netloc and parsed_url.netloc != self.base_domain:\n",
    "                continue\n",
    "\n",
    "            if absolute_url in (\"#\", \"/\", \".\") or link.startswith((\"#\", \"/\", \".\")):\n",
    "                continue\n",
    "\n",
    "            if any(parsed_url.path.lower().endswith(ext) for ext in EXCLUDED_EXTENSIONS):\n",
    "                continue\n",
    "\n",
    "            if not absolute_url.startswith((\"http://\", \"https://\")):\n",
    "                continue \n",
    "\n",
    "            valid_links.append(absolute_url)\n",
    "\n",
    "        return list(set(valid_links))  # Remove duplicates\n",
    "\n",
    "    def get_links(self):\n",
    "        'Returns clean, valid, internal links.'\n",
    "        return self.links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9092180-e229-4789-a74d-29f53b5a6f3c",
   "metadata": {},
   "source": [
    "# Selecting relevant Links with the help of LLAMA 3.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c5fd3ac-b976-4b73-b375-11640598c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af4c9642-2028-4de6-981b-976527a05242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links_with_llama(links):\n",
    "    \"relevant link selection\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping to create a company brochure. \n",
    "    Here is a list of web links:\n",
    "\n",
    "    {json.dumps(links, indent=2)}\n",
    "\n",
    "    Select only the most relevant links for the brochure, such as:\n",
    "    - About Us\n",
    "    - Vision & Mission\n",
    "    - Leadership (Director, Officers, etc.)\n",
    "    - Contact Information\n",
    "    - Investor Relations (if available)\n",
    "    - Careers (if available)\n",
    "    - Services/Products\n",
    "    Generate a **well-structured, engaging, and informative brochure** using this information.  \n",
    "    The brochure **should be formatted in Markdown** for improved readability, including:\n",
    "    - **Headings** (`#`, `##`, `###`)\n",
    "    - **Bullet points** (`-` or `*`)\n",
    "    - **Bold & Italics** for emphasis  \n",
    "    - **Horizontal dividers (`---`)** between different sections for better readability wherever needed.\n",
    "    - **Links** (if relevant)\n",
    "    Do NOT include placeholders like \"[Cover Page: ...]. Ensure the content flows naturally, as if designed for customers, investors, or stakeholders.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    # print(\"Raw Response:\", response.message.content)\n",
    "\n",
    "    links = re.findall(r'https?://\\S+', response.message.content)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9247ba0-1cd4-434d-8d9f-c9df0cc27923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\llms\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'abc.xyz'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Brochure Links: ['http://abc.xyz/investor)', 'http://abc.xyz/investor)', 'http://abc.xyz/investor)', 'http://abc.xyz/careers)']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://abc.xyz/\"\n",
    "scraper = WebScraper(url)\n",
    "extracted_links = scraper.links\n",
    "\n",
    "filtered_links = filter_links_with_llama(extracted_links)\n",
    "print(\"Filtered Brochure Links:\", filtered_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5d011-6a17-4ef4-950e-5631d56cdc2c",
   "metadata": {},
   "source": [
    "# Fetching content from selected links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ff6ca45-9332-4483-ac2a-c1ed4b5c3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_content(url, max_chars=50000):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        if soup.body:\n",
    "            for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                tag.decompose()\n",
    "            text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "            return text[:max_chars]  # Truncate if needed\n",
    "        return \"\"\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62994c9-5c50-475a-822b-6c15889bb04b",
   "metadata": {},
   "source": [
    "# Generating Brochure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe4f18cf-f632-4623-973a-575dce663c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brochure(company_name, links):\n",
    "    all_content = []\n",
    "    for link in links:\n",
    "        print(f\"Fetching content from: {link}\")\n",
    "        page_text = fetch_page_content(link)\n",
    "        if page_text:\n",
    "            all_content.append(f\"### Source: {link}\\n{page_text}\\n\")\n",
    "\n",
    "    # context length for llama 3.2 : 131072 \n",
    "    combined_text = \"\\n\".join(all_content)[:500000]  # ~500K characters (safe margin)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant tasked with generating a **concise and professional company brochure** for \"{company_name}\".\n",
    "    Below are extracts from various web pages related to the company.\n",
    "\n",
    "    Please generate a well-structured, engaging brochure suitable for customers, investors, and stakeholders. Ensure it should be professionally \n",
    "    written.\n",
    "    Here is the extracted content:\n",
    "    {combined_text}\n",
    "    Generate a **well-structured, engaging, and informative brochure** using this information.The brochure **should be formatted in Markdown** for improved readability.\n",
    "    **Do NOT include placeholders like \"[Cover Page: ...]\".**  **Ensure the content flows naturally, as if designed for customers, investors, or stakeholders.**  \n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6a246ab-3d6a-420f-9ffd-d5eeeae9c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from: http://abc.xyz/investor)\n",
      "Error fetching http://abc.xyz/investor): 404 Client Error: Not Found for url: https://abc.xyz/investor)\n",
      "Fetching content from: http://abc.xyz/investor)\n",
      "Error fetching http://abc.xyz/investor): 404 Client Error: Not Found for url: https://abc.xyz/investor)\n",
      "Fetching content from: http://abc.xyz/investor)\n",
      "Error fetching http://abc.xyz/investor): 404 Client Error: Not Found for url: https://abc.xyz/investor)\n",
      "Fetching content from: http://abc.xyz/careers)\n",
      "Error fetching http://abc.xyz/careers): 404 Client Error: Not Found for url: https://abc.xyz/careers)\n"
     ]
    }
   ],
   "source": [
    "company_name = \"Google\"\n",
    "links=filtered_links\n",
    "brochure_text = generate_brochure(company_name, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e28e49-cfae-40c5-abd3-17191dab8f67",
   "metadata": {},
   "source": [
    "# BROCHURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12964b33-ae98-409d-a643-8c3b8573c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77da3134-bc3e-4213-854e-8624a20d804f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Google\n",
       "## A Leader in Innovation and Technology\n",
       "\n",
       "At Google, our mission is to organize the world's information and make it universally accessible and useful. We achieve this through our cutting-edge technology, innovative products, and collaborative approach.\n",
       "\n",
       "### Our Story\n",
       "\n",
       "Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University. Initially, the company focused on developing a search engine that could retrieve relevant information from the web. Over time, Google evolved into a multinational technology company with a diverse range of products and services.\n",
       "\n",
       "### Our Products and Services\n",
       "\n",
       "*   **Google Search**: The world's most popular search engine, providing accurate and up-to-date results.\n",
       "*   **Google Ads**: A platform for businesses to reach their target audience through paid advertising.\n",
       "*   **Google Cloud**: A suite of cloud computing services designed to help organizations build, deploy, and manage applications.\n",
       "*   **Google Drive**: A cloud storage service allowing users to store and access files from anywhere.\n",
       "*   **Google Pixel**: A series of smartphones offering exceptional camera performance and timely software updates.\n",
       "\n",
       "### Our Values\n",
       "\n",
       "*   **Innovation**: We empower our employees to think creatively and develop innovative solutions.\n",
       "*   **Collaboration**: We foster a culture of teamwork and open communication.\n",
       "*   **Quality**: We strive for excellence in everything we do, from product development to customer support.\n",
       "*   **Integrity**: We operate with transparency, honesty, and accountability.\n",
       "\n",
       "### Our Impact\n",
       "\n",
       "At Google, we believe that technology can be a powerful force for good. We've made significant contributions to various industries, including:\n",
       "\n",
       "*   **Artificial Intelligence (AI)**: We're investing heavily in AI research and development, with the goal of creating products that improve people's lives.\n",
       "*   **Education**: Our initiatives focus on making quality education accessible to everyone, regardless of location or background.\n",
       "*   **Environmental Sustainability**: We're committed to reducing our carbon footprint and promoting environmentally friendly practices.\n",
       "\n",
       "### Join the Google Team\n",
       "\n",
       "If you share our passion for innovation and collaboration, we invite you to join our team. With opportunities in various fields, including engineering, product management, and sales, there's something for everyone at Google.\n",
       "\n",
       "### Stay Connected\n",
       "\n",
       "Want to stay up-to-date on the latest news and developments from Google? Follow us on social media or visit our official website to learn more about our products, services, and initiatives.\n",
       "\n",
       "*   [Twitter](https://twitter.com/google)\n",
       "*   [Facebook](https://www.facebook.com/google)\n",
       "*   [YouTube](https://www.youtube.com/google)\n",
       "\n",
       "We look forward to working with you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(brochure_text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27d990-b6a7-4c95-829b-87cf9fbf348c",
   "metadata": {},
   "source": [
    "# Note\n",
    "* Note that we have used LLAMA 3.2 trained on 3.2 Billion parameters.\n",
    "* So it may not be as powerful as GPT-4 model by open AI which is trained on 1.8 trillion parameters\n",
    "* When we use open AI api based model it gives results in different format with proper markdowns and headings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
